{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para avaliação do modelo\n",
    "def evaluate_model(y_test, y_pred, model_name):\n",
    "    print(f\"Resultados para {model_name}:\")\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    #print(f'Acurácia: {accuracy:.4f}')\n",
    "    print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "    print(\"Relatório de Classificação:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    #print(\"Matriz de Confusão:\")\n",
    "    #print(confusion_matrix(y_test, y_pred))\n",
    "    #print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Avaliando o modelo DecisionTreeClassifier\n",
    "#evaluate_model(y_test, y_pred_clf, \"DecisionTreeClassifier\")\n",
    "\n",
    "# Avaliando o modelo XGBoost\n",
    "#evaluate_model(y_test, y_pred_xgb, \"XGBoost\")\n",
    "\n",
    "# Avaliando o modelo Random Forest com Gini\n",
    "#evaluate_model(y_test, y_pred_rf, \"Random Forest com Gini\")\n",
    "\n",
    "\n",
    "# Função para avaliar múltiplos modelos e retornar o melhor desempenho\n",
    "def evaluate_models(models_dict):\n",
    "    best_model_name = None\n",
    "    best_accuracy = 0\n",
    "    best_results = {}\n",
    "\n",
    "    for model_name, (model, y_test, y_pred) in models_dict.items():\n",
    "        print(f\"Resultados para {model_name}:\")\n",
    "        \n",
    "        # Acurácia\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "        \n",
    "        # Relatório de Classificação\n",
    "        print(\"Relatório de Classificação:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        \n",
    "        # Matriz de Confusão\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        #print(\"Matriz de Confusão:\")\n",
    "        #print(conf_matrix)\n",
    "        \n",
    "        # Visualizando a Matriz de Confusão\n",
    "        #plt.figure(figsize=(10, 7))\n",
    "        #sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "        #            xticklabels=['Classe 0', 'Classe 1'], yticklabels=['Classe 0', 'Classe 1'])\n",
    "        #plt.title(f'Matriz de Confusão - {model_name}')\n",
    "        #plt.xlabel('Predição')\n",
    "        #plt.ylabel('Real')\n",
    "        #plt.show()\n",
    "        \n",
    "        # Armazenar os resultados para comparação\n",
    "        best_results[model_name] = {\n",
    "            'accuracy': accuracy,\n",
    "            'classification_report': classification_report(y_test, y_pred),\n",
    "            'conf_matrix': conf_matrix\n",
    "        }\n",
    "        \n",
    "        # Atualizar o melhor modelo\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model_name = model_name\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "    print(f\"Modelo com Melhor Desempenho: {best_model_name}\")\n",
    "    print(\"Acurácia: %.2f%%\" % (best_accuracy * 100.0))\n",
    "    \n",
    "    return best_model_name, best_results"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
